<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>


<configuration> 

  <property>
    <name>dfs.nameservices</name>
    <value>{{ hdfs_logical_name }}</value>
  </property>

  <property>
    <name>dfs.ha.namenodes.{{ hdfs_logical_name }}</name>
    <value>nn1,nn2</value>
  </property>

  <property>
    <name>dfs.namenode.rpc-address.{{ hdfs_logical_name }}.nn1</name>
    <value>{{ namenode_1 }}:{{ namenode_rpc_port }}</value>
  </property>

  <property>
    <name>dfs.namenode.rpc-address.{{ hdfs_logical_name }}.nn2</name>
    <value>{{ namenode_2 }}:{{ namenode_rpc_port }}</value>
  </property>

  <property>
    <name>dfs.namenode.http-address.{{ hdfs_logical_name }}.nn1</name>
    <value>{{ namenode_1 }}:{{ namenode_http_port }}</value>
  </property>

  <property>
    <name>dfs.namenode.http-address.{{ hdfs_logical_name }}.nn2</name>
    <value>{{ namenode_2 }}:{{namenode_http_port }}</value>
  </property>

  <property>
    <name>dfs.datanode.http.address</name>
    <value>0.0.0.0:{{ datanode_http_port }}</value>
    <description>The datanode http server address and port.</description>
  </property>

  <property>
    <name>dfs.datanode.address</name>
    <value>0.0.0.0:{{ datanode_port }}</value>
    <description>The datanode server address and port for data transfer.</description> 
  </property>

  <property>
    <name>dfs.datanode.ipc.address</name>
    <value>0.0.0.0:{{ datanode_ipc_port }}</value>
    <description>The datanode ipc server address and port.</description>
  </property>

  <property>
    <name>dfs.namenode.shared.edits.dir</name>
    <value>qjournal://{{ journalnode_1 }}:{{ journalnode_rpc_port }};{{ journalnode_2 }}:{{ journalnode_rpc_port }};{{ journalnode_3 }}:{{ journalnode_rpc_port }}/{{ hdfs_logical_name }}</value>
    <description>the URI which identifies the group of JNs where the NameNodes will write/read edits</description>
  </property>

  <property>
    <name>dfs.datanode.data.dir</name>
    <value>{% for dir in hdfs_datanode_data_dirs -%}
        {{ dir }}{{ ',' if not loop.last else '' }}
	       {%- endfor %} </value>
    <description>Determines where on the local filesystem an DFS data node should store its blocks. If this is a comma-delimited list of directories, then data will be stored in all named directories, typically on different devices. Directories that do not exist are ignored.</description>
  </property>

  <property>
    <name>dfs.namenode.name.dir</name>
    <value>{% for dir in hdfs_namenode_name_dirs -%}
	    {{ dir }}{{ ',' if not loop.last else '' }}
	      {%- endfor %}</value>
    <description>Determines where on the local filesystem the DFS name node should store the name table(fsimage). If this is a comma-delimited list of directories then the name table is replicated in all of the directories, for redundancy.</description>
  </property>

  <property>
    <name>dfs.client.failover.proxy.provider.{{ hdfs_logical_name }}</name>
    <value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value>
    <description>the Java class that HDFS clients use to contact the Active NameNode</description>
  </property>

  <property>
    <name>dfs.ha.fencing.methods</name>
    <value>shell(/bin/true)</value>
    <description>a list of scripts or Java classes which will be used to fence the Active NameNode during a failover</description>
  </property>

  <property>
    <name>dfs.journalnode.edits.dir</name>
    <value>{{ hadoop_var_prefix }}/jnedits</value>
    <description>the path where the JournalNode daemon will store its local state</description>
  </property>

  <property>
    <name>dfs.ha.automatic-failover.enabled</name>
    <value>true</value>
    <description>Whether automatic failover is enabled. See the HDFS High Availability documentation for details on automatic HA configuration.</description>
  </property>

  <property>
    <name>ha.zookeeper.quorum</name>
    <value>{{ zookeeper_quorum }}</value>
  </property>

  <property>
    <name>dfs.support.append</name>
    <value>true</value>
    <description>Does HDFS allow appends to files?</description>
  </property>

  <property>
    <name>dfs.journalnode.rpc-address</name>
    <value>0.0.0.0:{{ journalnode_rpc_port }}</value>
    <description>The JournalNode RPC server address and port</description>
  </property>

  <property>
    <name>dfs.journalnode.http-address</name>
    <value>0.0.0.0:{{ journalnode_http_port }}</value>
    <description></description>
  </property>

  <property>
    <name>dfs.namenode.accesstime.precision</name>
    <value>0</value>
    <description>The access time for HDFS file is precise upto this value. The default value is 1 hour. Setting a value of 0 disables access times for HDFS.</description>
  </property>

  <property>
    <name>dfs.namenode.datanode.registration.ip-hostname-check</name>
    <value>false</value>
    <description>If true (the default), then the namenode requires that a connecting datanode's address must be resolved to a hostname. If necessary, a reverse DNS lookup is performed. All attempts to register a datanode from an unresolvable address are rejected. It is recommended that this setting be left on to prevent accidental registration of datanodes listed by hostname in the excludes file during a DNS outage. Only set this to false in environments where there is no infrastructure to support reverse DNS lookup.</description>
  </property>

</configuration>
